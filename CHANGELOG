version-1.4 2020
    - add helper scripts grid_explore.sh and informaticResources.py 

version-1.3 2020-07-08
    - sorting is now stable

version-1.2 2020-06-29
    - fix a run time error 

version-1.1 2020-06-09
    - release the memory bounds when sorting with power of 2 MPI jobs
    - fix unbalanced final collective writing
    - add benchmark part in documentation

version-1.0 2020-03-25
    - mpiSORT: sorts SAM files

----------------------------------------------------------------------
Preliminary releases


Release from the 03/03/2020

1) rename output file when sorting results of mpiBWAByChr now it is chrX_discordant. It contains essentially the supplementary discordant alignments
2) remove memory pressure when sorting results of mpiBWAByChr. Now the memory needed is 2.5 the SAM size file.

Release from the 13/02/2020

1) Now only power are accepted for CPU number
2) Fix problem when sorting by name (get back the "\0")


Release from the 09/01/2020

1) remove character "\0" during parse paired

Release from the 31/03/2019

1) fix a corner case when reads distribution is unbalanced.

Release from the 28/06/2018

1) Creation of a google group
	https://groups.google.com/forum/#!forum/hpc-bioinformatics-pipeline

2) For improvement on Lustre file system removed the “nosuid” mount option.


Release from 30/04/2018

1) fix a memory leak with khash in write.c

Release from 16/02/2018

1) fix a bug in the offset of the first read.
Change the argument of init_goff function replace headerSize with hsiz in mpiSort.c.

2) next steps are:
    - Investigate why Valgrind complains about MPI_mem_alloc in bitonic sort part?
    - Marking duplicates

Release  from 30/08/2017

1) fix the condition for quality threshold. (parser.c line 179)
Now we include read with mapping quality equal to zero (when no quality is specified).

Release  from 14/04/2017

1) Add citations.
2) Improve the memory pressure.
3) Remove some assert and add comments.
4) Malloc_trim is no op on BSD or OSX.
5) First tests.

Release from 28/03/2017

1) We have redesigned the communications during the sorting.
Communication are optimized for power of 2 in processor number.
This has been done to scale gigabit ethernet network and for a better load balancing of the computing and memory pressure.
With processors in power of 2 we avoid extra communication.
All Results are the same than previous release.

New tests and benchmarking will follow.

2) Remove memory leaks.
Comments added.

Release from 25/11/2016

1) Bug fix at the beginning of the program during the parsing of the SAM.
2) Need to test if the MPI sctructures scale very large buffer (above 5Gb per job).

Release from 23/11/2016

1) Remove the limit in  MPI_file_read_at  of 1 Gb per read buffer.
SAM data are first stored in intermediate buffer and copy to the main SAM buffer see in mpiSORT.c (line 344)
2) Replace MPI_Create_struct with a local copy (in write.c line 2712)
3) Cleaning of the code and add comments
4) Tested with a beegfs scratch (4 MDS) and 10gb ethernet, it scales to 200Gb sam file and 80 cpu and takes 10mn for reading and 10mn sorting.

Release from 18/11/2016

1) Trimming of the memory after big or multiple free() with malloc_trim().
Efficient with Linux but not tested on BSD or IOX.
2) Remove memory leaks.
3) Add/remove  comments.

Release from 04/11/2016

1) Due to MPI packing overhead we replace with a local copy (write.c). See in future if MPI_Unpack is stable.
2) File extension becomes  .gz (not bam)
3) Discordant and unmapped are computed first.
4) Cleaning of memory.

Release from 11/10/2016

1) In this release we use the local data buffer for re-reading instead of the linux client cache.
Because for slow network the time to get the data back from the client or server cache can be long.
Now re-reading the data is intantaneous.
2) Add documentation and free some variables.

Release from 06/10/2016

1) The previous version didn't sort the offset destination before the shuffle. This bug is fixed.
2) New packaging with autools for distribution.

Release from 29/07/2016

1) Bug fix when data sample is too small and some jobs have nothing to sort
2) Bug fix when jobs under 6
3) We add new option -n for sorting by name
4) the parallel merge sort has been replaced with bitonic sorting (25% percent gain in speed-up)
5) Refactoring and cleaning of the code

